---
layout: post
title:  "Atlas元数据管理工具"
date:   2021-3-24
categories: big data
---

## 数据仓库元数据管理

元数据(MetaData)狭义的解释是用来描述数据的数据。广义的来看，除了业务逻 辑直接读写处理的那些业务数据，所有其它用来维持整个系统运转所需的信息/数据 都可以叫作元数据。如数据库中表的Schema信息，任务的血缘关系，用户和脚本/ 任务的权限映射关系信息等。  
  
管理元数据的目的，是为了让用户能够更高效的使用数据，也是为了让平台管理人员能更加有效的做好数据的维护管理工作。  
  
但通常这些元数据信息是散落在平台的各个系统，各种流程之中的，它们的管理也可能或多或少可以通过各种子系统自身的工具，方案或流程逻辑来实现。  
  
元数据管理平台很重要的一个功能就是信息的收集，至于收集哪些信息，取决于业务的需求和需要解决的目标问题。  
  
应该收集那些信息，没有绝对的标准，但是对大数据开发平台来说，常见的元数据信息包括:  
  * 表结构信息  
  * 数据的空间存储，读写记录，权限归属和其它各类统计信息  
  * 数据的血缘关系信息  
  * 数据的业务属性信息

- 数据血缘关系
  血缘信息或者叫做Lineage的血统信息是什么，简单的说就是数据之 间的上下游来源去向关系，数据从哪里来到哪里去。如果一个数据有问题，可以根据 血缘关系往上游排查，看看到底在哪个环节出了问题。此外也可以通过数据的血缘关 系，建立起生产这些数据的任务之间的依赖关系，进而辅助调度系统的工作调度，或 者用来判断一个失败或错误的任务可能对哪些下游数据造成影响等等。

- 数据的业务属性信息
  如一张数据表的统计口径信息，这 张表干什么用的，各个字段的具体统计方式，业务描述，业务标签，脚本逻辑的历史 变迁记录，变迁原因等，此外还包括对应的数据表格是由谁负责开发的，具体数据的 业务部门归属等。数据的业务属性信息，首先是为业务服务的，它的采集和展示也就 需要尽可能的和业务环境相融合，只有这样才能真正发挥这部分元数据信息的作用。

## Atlas简介
![](/resource/atlas/assets/E530606F-2A15-46F8-952A-08F8C4233274.png)

Atlas是Hadoop平台元数据框架;  
Atlas是一组可扩展的核心基础治理服务，使企业能够有效，高效地满足Hadoop中的 合规性要求，并能与整个企业数据生态系统集成;  
  
Apache Atlas为组织提供了开放的元数据管理和治理功能，以建立数据资产的目录， 对这些资产进行分类和治理，并为IT团队、数据分析团队提供围绕这些数据资产的协 作功能。  
  
Atlas由元数据的收集，存储和查询展示三部分核心组件组成。此外，还会有一个管 理后台对整体元数据的采集流程以及元数据格式定义和服务的部署等各项内容进行配 置管理。

- Core
  Atlas功能核心组件，提供元数据的获取与导出(Ingets/Export)、类型系统(Type System)、元数据存储索引查询等核心功能

- Integration
  Atlas对外集成模块。外部组件的元数据通过该模块将元数据交给Atlas管理

- Metadata source
  Atlas支持的元数据数据源，以插件形式提供。当前支持从以下来源提取和管理元数据: Hive, HBase, Sqoop, Kafka, Storm

- Applications
  Atlas的上层应用，可以用来查询由Atlas管理的元数据类型和对象

- Graph Engine(图计算引擎)
  Altas使用图模型管理元数据对象。图数据库提 供了极大的灵活性，并能有效处理元数据对象之间的关系。除了管理图对象之 外，图计算引擎还为元数据对象创建适当的索引，以便进行高效的访问。在 Atlas 1.0 之前采用Titan作为图存储引擎，从1.0开始采用 JanusGraph 作为图存 储引擎。JanusGraph 底层又分为两块:  
  * Metadata Store。采用 HBase 存储 Atlas 管理的元数据;   
  * Index Store。采用Solr存储元数据的索引，便于高效搜索;

## 安装配置

- 准备软件包
  需要使用 Maven 3.6.3 编译  
  软件包：  
  * apache-atlas-1.2.0-sources.tar.gz   
  * solr-5.5.1.tgz  
  * hbase-1.1.2.tar.gz

- 解压缩源码，修改配置
  ```shell 
  # 解压缩  
  cd /opt/lagou/software  
  tar zxvf apache-atlas-1.2.0-sources.tar.gz cd apache-atlas-sources-1.2.0/  
    
  # 修改配置   
  vi pom.xml  
    
  # 修改. hadoop 版本需与本地安装的一致  
  645 <npm-for-v2.version>3.10.8</npm-for-v2.version>   
  652 <hadoop.version>2.9.2</hadoop.version>  
  ```

- 将HBase、Solr的包拷贝到对应的目录中
  如果不拷贝这些包，就需要下载，下载 HBase 和 Solr 时速度很慢。这里提前下载完 所需的这两个组件，拷贝到对应目录中。  
    
  ```shell 
  cd /opt/lagou/software/apache-atlas-sources-1.2.0  
    
  # 创建目录  
  cd distro/   
  mkdir solr   
  mkdir hbase  
    
  # 拷贝软件包  
  cp /opt/lagou/software/solr-5.5.1.tgz ./solr/  
    
  cp /opt/lagou/software/hbase-1.1.2.tar.gz ./hbase/  
  ```

- maven设置阿里镜像
  ```shell 
  cd $MAVEN_HOME/conf  
    
    
  # 在配置文件中添加  
  vi settings.xml  
    
  #加在158行后   
  <mirror>  
        <id>alimaven</id>  
        <name>aliyun maven</name>  
  <url>http://maven.aliyun.com/nexus/content/groups/public/</url>  
        <mirrorOf>central</mirrorOf>  
     </mirror>  
  ```  
    
  如果编译中遇到找不到包的错误，可以暂时屏蔽阿里源，从maven官方源继续下载安装，后面运行太慢可以考虑再次切回阿里镜像源

- Atlas编译
  编译过程中大概要下载600M左右的jar，持续的时间比较长。  
    
  ```shell 
  cd /opt/lagou/software/apache-atlas-sources-1.2.0  
    
  export MAVEN_OPTS="-Xms2g -Xmx2g"  
    
  mvn clean -DskipTests package -Pdist,embedded-hbase-solr  
  ```  
    
  编译完的软件位置:/opt/lagou/software/apache-atlas-sources- 1.2.0/distro/target  
    
  编译完的软件:apache-atlas-1.2.0-bin.tar.gz

- Atlas安装
  ```shell 
  cd /opt/lagou/software/apache-atlas-sources-1.2.0/distro/target  
    
  # 解压缩  
  tar zxvf apache-atlas-1.2.0-bin.tar.gz  
  mv apache-atlas-1.2.0/ /opt/lagou/servers/atlas-1.2.0  
    
  # 修改 /etc/profile，设置环境变量 ATLAS_HOME  
    
  # 启动服务(第一次启动服务的时间比较长)   
  cd $ATLAS_HOME/bin   
  ./atlas_start.py  
    
  # 停止服务  
  ./atlas_stop.py  
  ```  
    
  Web服务:[http://localhost:21000/login.jsp](http://localhost:21000/login.jsp)  
    
  用户名 / 口令:admin / admin  
    
  账号的信息存储在文件 conf/users-credentials.properties 中。其中 Password 通过如下方式产生sha256sum 摘要信息:  
    
  ```shell 
  echo -n "admin" | sha256sum  
  ```  
    
  修改conf/users-credentials.properties中的信息，可以改密码

## Hive血缘关系导入

- 配置HIVE_HOME环境变量
  将 $ATLAS_HOME/conf/atlas-application.properties 拷贝到 $HIVE_HOME/conf 目录下  
    
  ```shell 
  ln -s $ATLAS_HOME/conf/atlas-application.properties $HIVE_HOME/conf/atlas-application.properties  
  ```

- 拷贝jar包
  使用如下脚本，一次性把所有可能用到的包都建立软链接  
    
  ```shell 
  source_path=“$ATLAS_HOME/server/webapp/atlas/WEB-INF/lib"  
  target_path="$ATLAS_HOME/hook/hive/atlas-hive-plugin-impl"  
    
  for jar in `ls $source_path`;  
  	do  
  		ln -s $source_path/$jar $target_path/$jar  
  done  
  ```  
    
  后面如果遇到class相关的冲突，再根据错误移除对应的包

- 修改Hive的配置
  hive-site.xml增加 hook  
    
  ```xml  
  <property>  
    <name>hive.exec.post.hooks</name>  
    <value>org.apache.atlas.hive.hook.HiveHook</value>  
  </property>  
  ```  
    
  $HIVE_HOME/conf/hive-env.sh中添加HIVE_AUX_JARS_PATH变量  
    
  ```shell 
  export HIVE_AUX_JARS_PATH=/opt/lagou/servers/atlas-1.2.0/hook/hive  
  ```

- 批量导入hive数据<br>
![](/resource/atlas/assets/74E52C9D-1519-4EAB-90F9-3744FB8B9631.png)
  前提： Hive能正常启动;在执行的过程中需要用户名/口令:admin/admin  
    
  ```shell 
  sh $ATLAS_HOME/bin/import-hive.sh  
  ```  
    
  成功导出可以看见最后的提示信息:Hive Meta Data imported successfully!!!   
  在浏览器中可以看见:Search 中的选项有变化  
    
    
  Hive hook 可捕获以下操作:  
  * create database  
  * create table/view,   
  * create table as select   
  * load, import, export  
  * DMLs (insert)  
  * alter database  
  * alter table  
  * alter view

- 血缘关系图<br>
![](/resource/atlas/assets/609090C4-76D2-4B19-9D74-98CD5D55286B.png)
  启动atlas之后，它就会记录hive中数据的流向

