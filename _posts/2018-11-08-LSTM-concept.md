---
layout: post
title:  "LSTM长短期记忆网络"
date:   2018-11-08
categories: concept
---

reference:
[Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

### 循环神经网络(Recurrent Neural Networks)
人类不会每次都从零开始思考.当你在读这篇文章的时候,你通过前面看过的字词理解每个新出现的词语.你不会忘记前面的所见而重新从零开始思考.你的思维有持续性.

传统的神经网络不能做到这些,这似乎是它的最大局限.例如,想象一下你想分辨电影中每个时间点发生的事情.传统神经网络如何用前一个场景去推导下一个场景,未可知.

循环神经网络为此而生.它是一个环状的网络,这让信息可以续存.

<br><img style="width:120px" src="/resource/lstm/RNN-rolled.png">

上图中,神经网络单元,A,输入 __*X<sub>t</sub>*__ 输出 __*h<sub>t</sub>*__.环状网络结构让信息从上一个网络单元传到下一个.

这个环让循环神经网络看上去有点神秘.然而,如果你再使仔细琢磨一下,它其实和普通神经网络没有很大的区别.一个循环神经网络可以看作一个相同的网络被复制了n多份,每个把信息传递给下一个.想象一下,如果我们把这个环展开:

![](/resource/lstm/RNN-unrolled.png)

这种链性特征显示出循环神经网络跟序列有紧密关联.它们是神经网络为了处理序列性数据而产生的结构.
这确实挺有用的!过去的几年,把RNN应用到各式各样的问题取得了令人不可置信的成功:语音识别,语言模型,翻译,图片描述...还不断有新的应用出现.
这些成功案例的关键在于使用了“LSTM”,循环神经网络的一种类型,它在处理很多任务时,比标准型效果好的要好非常非常多.几乎所有基于循环神经网络取得的卓越成就都是使用了LSTM.这篇文章将会详细探讨LSTM.

### 长期依赖的痛点
RNN 的作用在于它可能能够把前面的信息关联到当前任务中,比如使用前面的视频可能有助于理解当前的画面.如果RNN可以做到这些,它们将会极其有用.但是它可以吗?看情况.

有时候,我们只需要观察最近的信息就可以完成当前任务.举个例子,考虑一个语言模型通过前面的几个词预测下一个词.如果我们试图预测“白云飘在 __*天空*__”的最后一个词,我们不需要更多上下文 - 很明显下一个词会是 __*天空*__.诸如此类,需要的预测的信息和关联信息的间隙比较小,RNN可以学会如何使用过去的信息.
![](/resource/lstm/RNN-shorttermdepdencies.png)

但是别的场景我们可能需要更多的上下文.考虑一下预测这句话的最后一个,“我在法国长大...我说着流利的 __*法语*__.”最靠近的句子可以预测到下一个词可能是一种语言,但如果我们想知道是哪种语言,我们需要包含 _法国_ 的段落,这就是离得比较远了.完全有这样的可能,我们需要的信息会埋藏在很遥远的上文.

不幸的是,随着间隙变大,RNN开始无法学习信息之间的关联性.
![](/resource/lstm/RNN-longtermdependencies.png)

理论上,RNN完全有能耐处理这样的“长期依赖”任务.人类可以很仔细的挑选合适的参数,让它解决这种类型的问题.遗憾的是,事实上,RNN几乎不能习得这一技能.这个问题在这两篇论文<a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf">Hochreiter (1991) [German]</a> 和 <a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf">Bengio, et al. (1994)</a>中有深度的剖析,论文里边从比较根本的角度说明它的难点.

值得庆幸的是,LSTM没有这个问题!

### 长短期记忆网络(Long short term memory networks,LSTM)

长短期记忆网络 - 通常叫做“LSTM” - 是一种特殊类型的RNN,可以学习长期依赖.它由<a href="http://www.bioinf.jku.at/publications/older/2604.pdf">Hochreiter &amp; Schmidhuber (1997)</a>创建, 然后被很多人优化并逐渐流行起来.它各式各样的问题都表现很好,现在被广泛使用.

LSTM专门设计来处理长期依赖问题.记住时间间隔比较长的信息实际上是它们的默认表现.

所有循环神经网络都有重复的神经网络模块链.对于标准的RNN,它的重复模块会有一个非常简单结构,比如一个tanh层.
![](/resource/lstm/LSTM3-SimpleRNN.png)

LSTM也有一个链式结构,但是它的重复单元有不同的结构.它有四个神经网络层,用特殊的方式进行交互.
![](/resource/lstm/LSTM3-chain.png)

不用担心LSTM里边究竟发生了什么.我们将会一步一步拆解它的结构.现在,让我们先弄清每个标记都代表什么.
![](/resource/lstm/LSTM2-notation.png)

上图中,每条线携带一个完整的向量,从一个节点的输出到其他节点的输入.粉色的圆圈代表逐点运算,比如向量相加,黄色的方块是神经网络层.合并到一起的线代表数据合并,分叉的线代表它的数据被复制,并且复制的数据输出到不同的地方.

## LSTM的核心概念
LSTM的关键是单元状态,一条水平的线穿过图示的上方.

单元状态有点像运输带.它笔直的跑完整条链,只有极少的线性数据交换.很有可能数据仅仅是流过去,而没有什么变动.
![](/resource/lstm/LSTM3-C-line.png)
LSTM没有能力改删除或添加信息到单元状态,这有它的“门”管理着.

门是一种选择性的让数据通过的方式.它们由一个Sigmoid神经网还有一个逐点乘法运算组成.<br>
<br>
<img style="width:120px" src="/resource/lstm/LSTM3-gate.png">

Sigmoid层的输出在0到1之间,表示每个数据应该通过多大比例.0代表“什么都别想通过”,1代表”全部通过!”.

一个LSTM有三个这样的门,用来保护、控制单元状态.
