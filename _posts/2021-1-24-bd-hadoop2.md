---
layout: post
title:  "hadoop 2. HDFS分布式文件系统"
date:   2021-1-24
categories: big data
---

HDFS (全称:Hadoop Distribute File System，Hadoop 分布式文件系统)是 Hadoop 核心组 成，是分布式存储服务。  
  分布式文件系统横跨多台计算机，在大数据时代有着广泛的应用前景，它们为存储和处理超大规模  
数据提供所需的扩展能力。  
HDFS是分布式文件系统中的一种。


## HDFS的重要概念

HDFS 通过统一的命名空间目录树来定位文件; 另外，它是分布式的，由很多服务器联合起来实现 其功能，集群中的服务器有各自的角色(分布式本质是拆分，各司其职);

### 典型的 Master/Slave 架构

HDFS 的架构是典型的 Master/Slave 结构。  
HDFS集群往往是一个NameNode(HA架构会有两个NameNode,联邦机制)+多个DataNode组成;  
NameNode是集群的主节点，DataNode是集群的从节点。

### 分块存储(block机制)

HDFS 中的文件在物理上是分块存储(block)的，块的大小可以通过配置参数来规定; Hadoop2.x版本中默认的block大小是128M;

### 命名空间(NameSpace)

HDFS 支持传统的层次型文件组织结构。用户或者应用程序可以创建目录，然后将文件保存在这些目录里。文件系统名字空间的层次结构和大多数现有的文件系统类似:用户可以创建、删除、移动 或重命名文件。  
Namenode 负责维护文件系统的名字空间，任何对文件系统名字空间或属性的修改都将被 Namenode 记录下来。  
HDFS提供给客户单一个抽象目录树，访问形式:hdfs://namenode的hostname:port/test/input  
hdfs://linux121:9000/test/input

### NameNode元数据管理

我们把目录结构及文件分块位置信息叫做元数据。  
NameNode的元数据记录每一个文件所对应的block信息(block的id,以及所在的DataNode节点 的信息)

### DataNode数据存储

文件的各个 block 的具体存储管理由 DataNode 节点承担。一个block会有多个DataNode来存储，DataNode会定时向NameNode来汇报自己持有的block信息。

### 副本机制

为了容错，文件的所有 block 都会有副本。每个文件的 block 大小和副本系数都是可配置的。应用 程序可以指定某个文件的副本数目。副本系数可以在文件创建的时候指定，也可以在之后改变。 副本数量默认是3个。  
  
注意： 文件副本的实际数量与datanode的数量有关，比如设置文件副本数量为10，但是实际节点只有3，那文件的实际副本数量就是3.   
**文件的实际副本数量<=datanode（<=设置副本数）**

### 一次写入，多次读出

HDFS 是设计成适应一次写入，多次读出的场景，且不支持文件的随机修改。 (**支持追加写入， 不支持随机更新**)  
正因为如此，HDFS 适合用来做大数据分析的底层存储服务，并不适合用来做网盘等应用(修改不方便，延迟大，网络开销大，成本太高)

## HDFS 架构

![](/resource/hadoop2/assets/F39CEAFA-3077-42B4-A36F-0506A2EA0776.png)

### NameNode(nn):Hdfs集群的管理者，Master

* 维护管理Hdfs的名称空间(NameSpace)   
* 维护副本策略   
* 记录文件块(Block)的映射信息   
* 负责处理客户端读写请求

### DataNode:NameNode下达命令，DataNode执行实际操作，Slave节点。

* 保存实际的数据块  
* 负责数据块的读写

### Client:客户端 shell，web，java等

* 上传文件到HDFS的时候，Client负责将文件切分成Block,然后进行上传   
* 请求NameNode交互，获取文件的位置信息   
* 读取或写入文件，与DataNode交互   
* Client可以使用一些命令来管理HDFS或者访问HDFS

## HDFS 客户端操作

### Shell 命令行操作HDFS

- 基本语法

  ```sh  
  hadoop fs 具体命令 OR bin/hdfs dfs 具体命令  
  ```

- 命令大全

  ```sh  
  [root@centos7-1 ~]# hadoop fs  
  Usage: hadoop fs [generic options]  
  	[-appendToFile <localsrc> ... <dst>]  
  	[-cat [-ignoreCrc] <src> ...]  
  	[-checksum <src> ...]  
  	[-chgrp [-R] GROUP PATH...]  
  	[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]  
  	[-chown [-R] [OWNER][:[GROUP]] PATH...]  
  	[-copyFromLocal [-f] [-p] [-l] [-d] <localsrc> ... <dst>]  
  	[-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]  
  	[-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] <path> ...]  
  	[-cp [-f] [-p | -p[topax]] [-d] <src> ... <dst>]  
  	[-createSnapshot <snapshotDir> [<snapshotName>]]  
  	[-deleteSnapshot <snapshotDir> <snapshotName>]  
  	[-df [-h] [<path> ...]]  
  	[-du [-s] [-h] [-x] <path> ...]  
  	[-expunge]  
  	[-find <path> ... <expression> ...]  
  	[-get [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]  
  	[-getfacl [-R] <path>]  
  	[-getfattr [-R] {-n name | -d} [-e en] <path>]  
  	[-getmerge [-nl] [-skip-empty-file] <src> <localdst>]  
  	[-help [cmd ...]]  
  	[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [<path> ...]]  
  	[-mkdir [-p] <path> ...]  
  	[-moveFromLocal <localsrc> ... <dst>]  
  	[-moveToLocal <src> <localdst>]  
  	[-mv <src> ... <dst>]  
  	[-put [-f] [-p] [-l] [-d] <localsrc> ... <dst>]  
  	[-renameSnapshot <snapshotDir> <oldName> <newName>]  
  	[-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]  
  	[-rmdir [--ignore-fail-on-non-empty] <dir> ...]  
  	[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]  
  	[-setfattr {-n name [-v value] | -x name} <path>]  
  	[-setrep [-R] [-w] <rep> <path> ...]  
  	[-stat [format] <path> ...]  
  	[-tail [-f] <file>]  
  	[-test -[defsz] <path>]  
  	[-text [-ignoreCrc] <src> ...]  
  	[-touchz <path> ...]  
  	[-truncate [-w] <length> <path> ...]  
  	[-usage [cmd ...]]  
    
  Generic options supported are:  
  -conf <configuration file>        specify an application configuration file  
  -D <property=value>               define a value for a given property  
  -fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.  
  -jt <local|resourcemanager:port>  specify a ResourceManager  
  -files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster  
  -libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath  
  -archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines  
    
  The general command line syntax is:  
  command [genericOptions] [commandOptions]  
  ```

- 常用指令

	- hadoop fs -help rm
	  查看对应指令的用法

	- hadoop fs -ls /
	  显示目录信息

	- hadoop fs -mkdir -p /lagou/bigdata
	  在HDFS上创建目录

	- hadoop fs  -moveFromLocal  ./hadoop.txt  
	  /lagou/bigdata
	  从本地剪切粘贴到HDFS

	- hadoop fs -copyFromLocal README.txt /
	  从本地文件系统中拷贝文件到HDFS路径去

		- hadoop fs  -put  ./hadoop.txt  
		  /lagou/bigdata
		  从本地复制文件到HDFS

	- hadoop fs -copyToLocal /lagou/bigdata/hadoop.txt  
	  ./
	  从HDFS拷贝到本地

		- hadoop fs -get /lagou/bigdata/hadoop.txt  
		  ./
		  从HDFS拷贝到本地

	- hadoop fs -appendToFile hdfs.txt  
	  /lagou/bigdata/hadoop.txt
	  追加一个文件到已经存在的文件末尾

	- hadoop fs -cat /lagou/bigdata/hadoop.txt
	  显示文件内容

	- hadoop fs  -chmod  666  /lagou/bigdata/hadoop.txt
	  改变文件访问权限，同linux系统

	- hadoop fs  -chown  root:root  
	  /lagou/bigdata/hadoop.txt
	  改变文件从属的用户及群组

	- hadoop fs -chgrp root /test
	  改变文件夹的用户组

	- hadoop fs -cp /lagou/bigdata/hadoop.txt /hdfs.txt
	  从HDFS的一个路径拷贝到HDFS的另一个路径

	- hadoop fs -mv /hdfs.txt /lagou/bigdata/
	  在HDFS目录中移动文件，同一个目录中移动就是重命名

	- hadoop fs -tail /user/root/test/yarn.txt
	  显示一个文件的末尾

	- hadoop fs -head /user/root/test/yarn.txt
	  显示一个文件的开头

	- hadoop fs -rm /user/root/test/yarn.txt
	  删除文件或空文件夹  
	  删除文件夹 加 `-R`

	- hadoop fs -rmdir /test
	  删除空目录

	- hadoop fs -du -s -h /user/root/test
	  统计文件夹的大小信息  
	  -s 排序  
	  -h 人类可读的形式显示文件大小

	- hadoop fs -setrep 10 /lagou/bigdata/hadoop.txt
	  设置HDFS中文件的副本数量  
	    
	  **这里设置的副本数只是记录在NameNode的元数据中，是否真的会有这么多副本，还得看DataNode的  
	  数量。因为目前只有3台设备，最多也就3个副本，只有节点数的增加到10台时，副本数才能达到10。**

### JAVA客户端操作HDFS

- 环境准备

	- 1. 解压hadoop源码包到非中文目录

	- 2. 配置HADOOP_HOME环境变量

	  ```sh  
	  # hadoop  
	  export HADOOP_HOME=/Users/april/java/hadoop-2.9.2  
	  export PATH=$HADOOP_HOME/bin:$PATH  
	  ```

	- 3. 创建maven工程，导入坐标
	
	  ```xml  
	  <dependencies>  
	  <!--        测试组件-->  
	          <dependency>  
	              <groupId>junit</groupId>  
	              <artifactId>junit</artifactId>  
	              <version>RELEASE</version>  
	          </dependency>  
	  <!--        输出log的插件-->  
	          <dependency>  
	              <groupId>org.apache.logging.log4j</groupId>  
	              <artifactId>log4j-core</artifactId>  
	              <version>2.8.2</version>  
	          </dependency>  
	  <!--        hadoop通用组件-->  
	          <dependency>  
	              <groupId>org.apache.hadoop</groupId>  
	              <artifactId>hadoop-common</artifactId>  
	              <version>2.9.2</version>  
	          </dependency>  
	  <!--        hadoop客户端组件-->  
	          <dependency>  
	              <groupId>org.apache.hadoop</groupId>  
	              <artifactId>hadoop-client</artifactId>  
	              <version>2.9.2</version>  
	          </dependency>  
	  <!--        hadoop hdfs文件系统组件-->  
	          <dependency>  
	              <groupId>org.apache.hadoop</groupId>  
	              <artifactId>hadoop-hdfs</artifactId>  
	              <version>2.9.2</version>  
	          </dependency>  
	      </dependencies>  
	  ```

	- 4.log4j配置文件
	  添加`log4j.properties`到`resource`目录下  
	    
	  ```sh  
	  log4j.rootLogger=INFO, stdout  
	  log4j.appender.stdout=org.apache.log4j.ConsoleAppender  
	  log4j.appender.stdout.layout=org.apache.log4j.PatternLayout  
	  log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n  
	  log4j.appender.logfile=org.apache.log4j.FileAppender  
	  log4j.appender.logfile.File=target/spring.log  
	  log4j.appender.logfile.layout=org.apache.log4j.PatternLayout  
	  log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n  
	  ```

- 快速入门案例

  ```java  
      @Test  
      public void testMkDirs() throws IOException, URISyntaxException, InterruptedException {  
          // 1 获取文件系统  
                  // 配置在集群上运行  
          Configuration configuration = new Configuration();  
  //        configuration.set("fs.defaultFS", "hdfs://centos7-1:9000");  
  //        configuration.set("dfs.replication", "2");  
  //        FileSystem fileSystem = FileSystem.get(configuration); //使用这种方式无法设置用户，会使用系统但前用户名，可能会出现访问权限问题  
          FileSystem  fileSystem = FileSystem.get(new URI("hdfs://centos7-1:9000"), configuration, "root");  
    
          // 2 创建目录  
          boolean mkdirs = fileSystem.mkdirs(new Path("/testJAVA"));  
          System.out.println("mkdirs=" + mkdirs);  
    
          // 关闭文件系统，释放资源  
          fileSystem.close();  
      }  
  ```

- HDFS文件系统权限问题
  * hdfs的文件权限机制与linux系统的文件权限机制类似!!  
  * r:read w:write x:execute 权限x对于文件表示忽略，对于文件夹表示是否有权限访问其内容  
  * 如果linux系统用户zhangsan使用hadoop命令创建一个文件，那么这个文件在HDFS当中的owner 就是zhangsan  
  * HDFS文件权限的目的，防止好人做错事，而不是阻止坏人做坏事。HDFS相信你告诉我你是谁， 你就是谁!!

	- 指定用户信息获取FileSystem对象

	  ```java  
	  FileSystem  fileSystem = FileSystem.get(new URI("hdfs://centos7-1:9000"), configuration, "root");  
	  ```

	- 关闭HDFS集群权限校验

	  ```xml  
	  vim hdfs-site.xml #添加如下属性  
	    
	  <property>  
	  	<name>dfs.permissions</name>  
	  	<value>true</value>  
	  </property>  
	  ```

	- 放弃HDFS的权限校验
	  基于HDFS权限本身比较鸡肋的特点，我们可以彻底放弃HDFS的权限校验，如果生产环境中 我们可以考虑借助kerberos以及sentry等安全框架来管理大数据集群安全。所以我们直接修 改HDFS的根目录权限为777  
	    
	  ```sh  
	  hadoop fs -chmod -R 777 /  
	  ```

- FileSystem
  hadoop文件管理系统相等于shell里边的 `fs` 指令

	- mkdirs(Path f)
	  创建文件夹

	- copyFromLocalFile(Path src, Path dst)
	  上传文件

		- dfs.replication的设置
		  副本数的设置：   
		    
		  1. 在代码中  
		    
		  ```java  
		    
		  Configuration configuration = new Configuration();  
		  configuration.set("dfs.replication", "2");  
		  ```  
		    
		  2. `resource->hdfs-site.xml`中  
		    
		  ```xml  
		  <?xml version="1.0" encoding="UTF-8"?>  
		  <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>  
		  <configuration>  
		      <property>  
		          <name>dfs.replication</name>  
		          <value>1</value>  
		      </property>  
		  </configuration>  
		  ```  
		    
		  3. `hdfs-defualt.xml`  
		    
		  ```xml  
		      <property>  
		          <name>dfs.replication</name>  
		          <value>3</value>  
		      </property>  
		  ```  
		    
		  **参数优先级排序:(1)代码中设置的值 >(2)用户自定义配置文件 >(3)服务器的默认配置**

	- copyToLocalFile(Path src, Path dst)
	  下载文件

	- delete(Path f)
	  删除文件/文件夹

	- RemoteIterator<LocatedFileStatus> listFiles(final Path f, final boolean recursive)
	  查看文件名称、权限、长度、块信息

		- LocatedFileStatus
		  文件信息封装类

			- Path getPath()

				- String getName()

			- long getLen()

			- boolean isFile()
			  是否文件

			- boolean isDirectory()
			  是否文件夹

			- ...

- IOUtils
  HDFS - IO流工具类，用来上传和下载文件

	- 上传文件

	  ```java  
	  // 1 创建输入流  
	  FileInputStream fis = new FileInputStream(new File("e:/lagou.txt"));   
	    
	  // 2 获取输出流  
	  FSDataOutputStream fos = fs.create(new Path("/lagou_io.txt"));   
	    
	  // 3 流拷贝  
	      IOUtils.copyBytes(fis, fos, configuration);  
	    
	  // 4 关闭资源 IOUtils.closeStream(fos); IOUtils.closeStream(fis);   
	    
	  fs.close();  
	  ```

	- 下载文件

	  ```java  
	  // 1 获取输入流  
	      FSDataInputStream fis = fs.open(new Path("/lagou_io.txt"));  
	    
	  // 2 获取输出流  
	      FileOutputStream fos = new FileOutputStream(new  
	  File("e:/lagou_io_copy.txt"));  
	    
	  // 3 流的对拷  
	  IOUtils.copyBytes(fis, fos, configuration);  
	    
	  // 4 关闭资源 IOUtils.closeStream(fos); IOUtils.closeStream(fis);   
	  fs.close();  
	  ```

	- seek调整读取文件的进度
	  读取文件，输出两次  
	    
	  ```java  
	  // 打开输入流,读取数据输出到控制台 FSDataInputStream in = null;   
	    
	  try{  
	  	in= fs.open(new Path("/lagou.txt"));   
	  	IOUtils.copyBytes(in, System.out, 4096, false);   
	  	in.seek(0); //从头再次读取  
	          IOUtils.copyBytes(in, System.out, 4096, false);  
	          }finally {  
	              IOUtils.closeStream(in);  
	          }  
	  ```

## HDFS读写解析

### HDFS读数据流程

- 
![](/resource/hadoop2/assets/3FE6C9C7-C8FF-49D2-AC88-3778A594154B.png)
  1. 客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据， 找到文件块所在的DataNode地址。  
  2. 挑选一台DataNode(就近原则[就近是指网络距离的就近]，然后随机)服务器，请求读取数据。  
  3. DataNode开始传输数据给客户端(从磁盘里面读取数据输入流，以Packet为单位来做校验)。  
  4. 客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。

###  HDFS写数据流程

- 
![](/resource/hadoop2/assets/63ADF7FB-ED75-4AD4-BF85-1599450124BC.png)
  1. 客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件 是否已存在，父目录是否存在。  
  2. NameNode返回是否可以上传。  
  3. 客户端请求第一个 Block上传到哪几个DataNode服务器上。  
  4. NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。  
  5. 客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。  
  6. dn1、dn2、dn3逐级应答客户端。  
  7. 客户端开始往dn1上传第一个Block(先从磁盘读取数据放到一个本地内存缓存)，以Packet为单  
  位，dn1收到一个Packet就会传给dn2，dn2传给dn3;dn1每传一个packet会放入一个确认队列等待确认。  
  8. 当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。(重复执行3-7步)。

### 验证每个packet是64k

```java  
@Test  
public void testUploadPacket() throws IOException {  
  
//1 准备读取本地文件的输入流  
final FileInputStream in = new FileInputStream(new  
File("e:/lagou.txt"));  
  
//2 准备好写出数据到hdfs的输出流  
final FSDataOutputStream out = fs.create(new Path("/lagou.txt"), new  
Progressable() {  
  
public void progress() { //这个progress方法就是每传输64KB(packet)就会执行一次  
// 第一次建立链接会进来一次，所以进来这个方法的次数 文件大小 / 64kb + 1  
  
 System.out.println("&");  
  
} });  
  
//3 实现流拷贝  
IOUtils.copyBytes(in, out, configuration); //默认关闭流选项是true，所以会自动关闭  
  
//4 关流  可以再次关闭也可以不关了   
  
}  
```

## NameNode与SecondNameNode

### 元数据管理流程图<br>
![](/resource/hadoop2/assets/D10BEF6D-5DE7-4FDC-8B63-359AA836F7E4.png)

### 第一阶段:NameNode启动

* 第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。  
* 客户端对元数据进行增删改的请求。  
* NameNode记录操作日志，更新滚动日志。  
* NameNode在内存中对数据进行增删改。

### 第二阶段:Secondary NameNode工作

* Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否执 行检查点操作结果。  
* Secondary NameNode请求执行CheckPoint。  
* NameNode滚动正在写的Edits日志。  
* 将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。   
* Secondary NameNode加载编辑日志和镜像文件到内存，并合并。   
* 生成新的镜像文件fsimage.chkpoint。   
* 拷贝fsimage.chkpoint到NameNode。   
* NameNode将fsimage.chkpoint重新命名成fsimage。

### Fsimage与Edits文件解析

- NameNode在执行格式化之后，会在/opt/lagou/servers/hadoop-2.9.2/data/tmp/dfs/name/current  
  目录下产生如下文件<br>
![](/resource/hadoop2/assets/46C1D778-35A9-4F8B-9ADD-059856D19364.png)

	- Fsimage文件
	  是namenode中关于元数据的镜像，一般称为检查点，这里包含了HDFS文件系统所有目录以及文件相关信息(Block数量，副本数量，权限等信息)

		- 查看内容
		  [官网地址](https://hadoop.apache.org/docs/r2.9.2/hadoop-project-dist/hadoop-hdfs/HdfsImageViewer.html)  
		    
		  把fsimge文件编译成可读文件  

		  ```sh  
		  # hdfs oiv -p 文件类型(xml) -i 镜像文件 -o 转换后文件输出路径  
		    
		  hdfs oiv -p XML -i fsimage_0000000000000000265 -o  
		  /opt/lagou/servers/fsimage.xml  
		  ```

		- Fsimage中为什么没有记录块所对应DataNode?
		  在内存元数据中是有记录块所对应的dn信息，但是fsimage中就剔除了这个信息;HDFS集群在启动的 时候会加载image以及edits文件，block对应的dn信息都没有记录，集群启动时会有一个安全模式 (safemode),安全模式就是为了让dn汇报自己当前所持有的block信息给nn来补全元数据。后续每隔一段时间dn都要汇报自己持有的block信息。

	- Edits文件
	  存储了客户端对HDFS文件系统所有的更新操作记录，Client对HDFS文件系统所有的 更新操作都会被记录到Edits文件中(不包括查询操作)

		- 文件内容
		  把edit文件编译成可读文件  
		    
		  ```sh  
		  # hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径  
		    
		  hdfs oev -p XML -i edits_0000000000000000266-  
		  0000000000000000267 -o /opt/lagou/servers/hadoop-2.9.2/edits.xml  
		  ```  
		    
		  Edits中只记录了更新相关的操作，查询或者下载文件并不会记录在内!!

		- NameNode启动时如何确定加载哪些Edits文件呢?
		  nn启动时需要加载fsimage文件以及那些没有被2nn进行合并的edits文件，nn如何判断哪些edits已经 被合并了呢?  
		  可以通过fsimage文件自身的编号来确定哪些已经被合并。edits文件的编号小于等于最大的fsimg编号说明已经被2nn合并了

	- seen_txid
	  该文件是保存了一个数字，数字对应着最后一个Edits文件名的数字

	- VERSION
	  该文件记录namenode的一些版本号信息，比如:CusterId,namespaceID等

- checkpoint周期
  `hdfs-default.xml`  
    
  ```xml  
  <!— 定时一小时 -->   
  <property> 		<name>dfs.namenode.checkpoint.period</name>  
  	 <value>3600</value>  
  </property>  
    
  <!-- 一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次 -->   
    
  <property> <name>dfs.namenode.checkpoint.txns</name>  
  <value>1000000</value> <description>操作动作次数</description>   
  </property>  
    
  <property>  
  <name>dfs.namenode.checkpoint.check.period</name>  
   <value>60</value>  
  <description> 1分钟检查一次操作次数</description> </property >  
  ```

## Namenode故障处理

NameNode故障后，HDFS集群就无法正常工作，因为HDFS文件系统的元数据需要由NameNode来管 理维护并与Client交互，如果元数据出现损坏和丢失同样会导致NameNode无法正常工作进而HDFS文 件系统无法正常对外提供服务。  
  
如果元数据出现丢失损坏如何恢复呢?

### 1. 将2NN的元数据拷贝到NN的节点下

此种方式会存在元数据的丢失。

### 2. 搭建HDFS的HA(高可用)集群

解决NN的单点故障问题!!(借助Zookeeper实现HA，一个  
Active的NameNode,一个是Standby的NameNode)

## 高级指令

### HDFS文件限额配置

HDFS文件的限额配置允许我们以文件大小或者文件个数来限制我们在某个目录下上传的文件数量或者文件内容总量，以便达到我们类似百度网盘网盘等限制每个用户允许上传的最大的文件的量

- 数量限额

  ```sh  
  hdfs dfs -mkdir -p /user/root/lagou  
  hdfs dfsadmin -setQuota 2 /user/root/lagou 个文件，上传文件，发现只能上传一个文件  
  hdfs dfsadmin -clrQuota /user/root/lagou  
  #创建hdfs文件夹  
  # 给该文件夹下面设置最多上传两  
  # 清除文件数量限制  
  ```

- 空间大小限额

  ```sh  
  hdfs dfsadmin -setSpaceQuota 4k /user/root/lagou # 限制空间大小4KB   
    
  #上传超过4Kb的文件大小上去提示文件超过限额  
  hdfs dfs -put /export/softwares/xxx.tar.gz /user/root/lagou  
    
  hdfs dfsadmin -clrSpaceQuota /user/root/lagou #清除空间限额  
    
  #查看hdfs文件限额数量  
  hdfs dfs -count -q -h /user/root/lagou  
  ```

### HDFS的安全模式

安全模式是HDFS所处的一种特殊状态，在这种状态下，文件系统只接受读数据请求，而不接 受删除、修改等变更请求。在NameNode主节点启动时，HDFS首先进入安全模式，DataNode在 启动的时候会向NameNode汇报可用的block等状态，当整个系统达到安全标准时，HDFS自动离 开安全模式。如果HDFS处于安全模式下，则文件block不能进行任何的副本复制操作，因此达到最小的副本数量要求是基于DataNode启动时的状态来判定的，启动时不会再做任何复制(从而达 到最小副本数量要求)，HDFS集群刚启动的时候，默认30S钟的时间是出于安全期的，只有过了 30S之后，集群脱离了安全期，然后才可以对集群进行操作。

- `hdfs dfsadmin  -safemode`

	- enter

	- leave

### Hadoop归档技术

主要解决HDFS集群存在大量小文件的问题!!  
由于大量小文件会占用NameNode的内存，因此对于HDFS来说存储大量小文件造成NameNode 内存资源的浪费!  
Hadoop存档文件HAR文件，是一个更高效的文件存档工具，HAR文件是由一组文件通过archive 工具创建而来，在减少了NameNode的内存使用的同时，可以对文件进行透明的访问，多个小文件合并成HAR文件之后, NameNode的元数据只会存HAR文件相关的信息,可以有效的减少NameNode内存的使用，对于实际操作处理文件依然是一个一个独立的文件。

- 
![](/resource/hadoop2/assets/4961E325-5F82-4ED4-B586-D86237EA9B4A.png)

- 1. 启动YARN集群

  ```sh  
  start-yarn.sh  
  ```

- 2. 归档文件

  ```sh  
  bin/hadoop archive -archiveName input.har –p  
  /user/root/input  /user/root/output  
  ```

- 3. 查看归档

  ```sh  
  # 查看har文件内部结构  
  hadoop fs -ls -R /user/root/output/input.har  
    
  # 查看har中归档的文件  
  hadoop fs -ls -R  
  [har:///user/root/output/input.har](har:///user/root/output/input.har)  
    
  ```

- 4. 解归档文件

  ```sh  
  hadoop fs -cp har:/// user/root/output/input.har/*  /user/root  
    
  hadoop fs -cat har:///output/input.har/img221.xml  
  ```  
    
  使用的时候就和其他文件夹一样使用，不过路径要加har://

